import .ReverseDiff
"""
    createcache(sp::DensePattern, dtype::ReverseAD, func!, nx, ng)

Cache for dense jacobian using reverse-mode AD.

# Arguments
- `func!::function`: function of form: f = func!(g, x)
- `nx::Int`: number of design variables
- `ng::Int`: number of constraints
"""
function createcache(sp::DensePattern, dtype::ReverseAD, func!, nx, ng)

    function combine!(fg, x)
        fg[1] = func!(@view(fg[2:end]), x)
    end

    g = zeros(1 + ng)
    x = zeros(nx)

    f_tape = ReverseDiff.JacobianTape(combine!, g, x)
    cache = ReverseDiff.compile(f_tape)
    J = DiffResults.JacobianResult(g, x)

    return DenseCache(combine!, g, J, cache, dtype)
end

"""
    evaluate!(g, df, dg, x, cache::DenseCache{T1,T2,T3,T4,T5} where {T1,T2,T3,T4,T5<:ReverseAD})

evaluate function and derivatives for a dense jacobian with reverse-mode AD

# Arguments
- `g::Vector{Float}`: constraints, modified in place
- `df::Vector{Float}`: objective gradient, modified in place
- `dg::Vector{Float}`: constraint jacobian, modified in place (order specified by sparsity pattern)
- `x::Vector{Float}`: design variables, input
- `cache::DenseCache`: cache generated by `createcache`
"""
function evaluate!(g, df, dg, x, cache::DenseCache{T1,T2,T3,T4,T5} 
    where {T1,T2,T3,T4,T5<:ReverseAD})
   
    ReverseDiff.jacobian!(cache.Jwork, cache.cache, x)
    fg = DiffResults.value(cache.Jwork)  # reference not copy
    J = DiffResults.jacobian(cache.Jwork)  # reference not copy
    f = fg[1]
    g[:] = fg[2:end]
    df[:] = J[1, :]
    dg[:] = J[2:end, :][:]
    
    return f
end

"""
    gradientcache(dtype::ReverseAD, func!, nx, ng)

Cache for gradient using ReverseDiff

# Arguments
- `func!::function`: function of form: f = func!(g, x)
- `nx::Int`: number of design variables
- `ng::Int`: number of constraints
"""
function gradientcache(dtype::ReverseAD, func!, nx, ng)

    function obj(x)
        return func!(zeros(eltype(x[1]), ng), x)
    end

    f_tape = ReverseDiff.GradientTape(obj, zeros(nx))
    cache = ReverseDiff.compile(f_tape)

    return GradOrJacCache(func!, nothing, cache, dtype)
end

"""
    gradient!(df, x, cache::GradOrJacCache{T1,T2,T3,T4} where {T1,T2,T3,T4<:ReverseAD})

evaluate gradient using ReverseDiff

# Arguments
- `df::Vector{Float}`: objective gradient, modified in place
- `x::Vector{Float}`: design variables, input
- `cache::DenseCache`: cache generated by `gradientcache`
"""
function gradient!(df, x, cache::GradOrJacCache{T1,T2,T3,T4}
    where {T1,T2,T3,T4<:ReverseAD})

    ReverseDiff.gradient!(df, cache.cache, x)

    return nothing
end
